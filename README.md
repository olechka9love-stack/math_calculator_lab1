# math_calculator_lab1
–õ–µ–∫—Å–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏ —Ñ—É–Ω–∫—Ü–∏–π
# save_as: create_project.py
import os
import sys

def create_project():
    """–°–æ–∑–¥–∞–µ—Ç –≤—Å—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–æ–µ–∫—Ç–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏"""
    
    # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞
    project_structure = {
        'grammar/Arithmetic.g4': '''grammar Arithmetic;

// –õ–µ–∫—Å–µ–º—ã
NUMBER    : [0-9]+ ('.' [0-9]+)?;
ID        : [a-zA-Z_][a-zA-Z_0-9]*;
PLUS      : '+';
MINUS     : '-';  
MUL       : '*';
DIV       : '/';
ASSIGN    : '=';
LPAREN    : '(';
RPAREN    : ')';
SEMI      : ';';
WS        : [ \t\r\n]+ -> skip;

// –°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞
program   : statement+ EOF;
statement : expr SEMI
          | ID ASSIGN expr SEMI;
   
expr      : term ( (PLUS | MINUS) term )*;
term      : factor ( (MUL | DIV) factor )*;
factor    : NUMBER
          | ID
          | ID LPAREN expr (',' expr)* RPAREN
          | LPAREN expr RPAREN;''',
        
        'src/__init__.py': '',
        
        'src/tokens.py': '''from enum import Enum
from typing import Any

class TokenType(Enum):
    # –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
    EOF = 'EOF'
    NUMBER = 'NUMBER'
    ID = 'ID'
    
    # –û–ø–µ—Ä–∞—Ç–æ—Ä—ã
    PLUS = 'PLUS'
    MINUS = 'MINUS'
    MUL = 'MUL'
    DIV = 'DIV'
    ASSIGN = 'ASSIGN'
    
    # –°–∫–æ–±–∫–∏ –∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏
    LPAREN = 'LPAREN'
    RPAREN = 'RPAREN'
    SEMI = 'SEMI'

class Token:
    def __init__(self, type: TokenType, value: Any = None, line: int = 1, column: int = 1):
        self.type = type
        self.value = value
        self.line = line
        self.column = column
    
    def __repr__(self):
        if self.value is not None:
            return f"Token({self.type.name}, {repr(self.value)})"
        return f"Token({self.type.name})"
    
    def __eq__(self, other):
        if not isinstance(other, Token):
            return False
        return (self.type == other.type and 
                self.value == other.value)''',
        
        'src/lexer.py': '''from src.tokens import Token, TokenType

class Lexer:
    def __init__(self, text: str):
        self.text = text
        self.pos = 0
        self.current_char = self.text[0] if text else None
        self.line = 1
        self.column = 1
    
    def error(self, message: str):
        raise Exception(f"Lexer error at line {self.line}, column {self.column}: {message}")
    
    def advance(self):
        """–ü–µ—Ä–µ–º–µ—â–∞–µ–º—Å—è –∫ —Å–ª–µ–¥—É—é—â–µ–º—É —Å–∏–º–≤–æ–ª—É"""
        if self.current_char == '\\n':
            self.line += 1
            self.column = 1
        elif self.current_char == '\\r':
            if self.pos + 1 < len(self.text) and self.text[self.pos + 1] == '\\n':
                self.pos += 1
                self.current_char = self.text[self.pos] if self.pos < len(self.text) else None
            self.line += 1
            self.column = 1
        else:
            self.column += 1
            
        self.pos += 1
        if self.pos >= len(self.text):
            self.current_char = None
        else:
            self.current_char = self.text[self.pos]
    
    def skip_whitespace(self):
        """–ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–±–µ–ª—ã, —Ç–∞–±—É–ª—è—Ü–∏–∏ –∏ –ø–µ—Ä–µ–≤–æ–¥—ã —Å—Ç—Ä–æ–∫"""
        while self.current_char is not None and self.current_char in ' \\t\\r\\n':
            self.advance()
    
    def number(self):
        """–ß–∏—Ç–∞–µ–º —á–∏—Å–ª–æ (—Ü–µ–ª–æ–µ –∏–ª–∏ –¥–µ—Å—è—Ç–∏—á–Ω–æ–µ)"""
        result = ''
        start_column = self.column
        
        while self.current_char is not None and self.current_char.isdigit():
            result += self.current_char
            self.advance()
        
        if self.current_char == '.':
            result += self.current_char
            self.advance()
            while self.current_char is not None and self.current_char.isdigit():
                result += self.current_char
                self.advance()
        
        if result.endswith('.'):
            self.error("Invalid number format")
            
        return Token(TokenType.NUMBER, float(result), self.line, start_column)
    
    def identifier(self):
        """–ß–∏—Ç–∞–µ–º –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä"""
        result = ''
        start_column = self.column
        
        while self.current_char is not None and (self.current_char.isalnum() or self.current_char == '_'):
result += self.current_char
            self.advance()
        
        return Token(TokenType.ID, result, self.line, start_column)
    
    def get_next_token(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞"""
        while self.current_char is not None:
            if self.current_char in ' \\t\\r\\n':
                self.skip_whitespace()
                continue
            
            if self.current_char.isdigit():
                return self.number()
            
            if self.current_char.isalpha() or self.current_char == '_':
                return self.identifier()
            
            start_column = self.column
            
            if self.current_char == '+':
                token = Token(TokenType.PLUS, '+', self.line, start_column)
                self.advance()
                return token
            
            if self.current_char == '-':
                token = Token(TokenType.MINUS, '-', self.line, start_column)
                self.advance()
                return token
            
            if self.current_char == '*':
                token = Token(TokenType.MUL, '*', self.line, start_column)
                self.advance()
                return token
            
            if self.current_char == '/':
                token = Token(TokenType.DIV, '/', self.line, start_column)
                self.advance()
                return token
            
            if self.current_char == '=':
                token = Token(TokenType.ASSIGN, '=', self.line, start_column)
                self.advance()
                return token
            
            if self.current_char == '(':
                token = Token(TokenType.LPAREN, '(', self.line, start_column)
                self.advance()
                return token
            
            if self.current_char == ')':
                token = Token(TokenType.RPAREN, ')', self.line, start_column)
                self.advance()
                return token
            
            if self.current_char == ';':
                token = Token(TokenType.SEMI, ';', self.line, start_column)
                self.advance()
                return token
            
            self.error(f"Unexpected character: '{self.current_char}'")
        
        return Token(TokenType.EOF, None, self.line, self.column)
    
    def tokenize(self):
        """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –≤—Å–µ—Ö —Ç–æ–∫–µ–Ω–æ–≤"""
        while True:
            token = self.get_next_token()
            yield token
            if token.type == TokenType.EOF:
                break''',
        
        'src/main.py': '''import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.lexer import Lexer

def print_tokens(text: str):
    """–í—ã–≤–æ–¥–∏—Ç —Ç–æ–∫–µ–Ω—ã –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
    lexer = Lexer(text)
    print(f"–ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥: {repr(text)}")
    print("–¢–æ–∫–µ–Ω—ã:")
    print("-" * 50)
    
    for i, token in enumerate(lexer.tokenize()):
        line_col = f"[{token.line}:{token.column}]"
        if token.type.name == 'EOF':
            print(f"{i:3}. {line_col:8} {token.type.name}")
            break
        
        value_str = f" = {repr(token.value)}" if token.value is not None else ""
        print(f"{i:3}. {line_col:8} {token.type.name:10}{value_str}")
    print()

def main():
    print("=" * 60)
    print("–ö–µ–π—Å 001: –õ–µ–∫—Å–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–∏—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π")
    print("=" * 60)
    print()
    
    test_cases = [
        "x = 5;",
        "y = 3.14 * (2 + x);",
        "a = 10 + 20 * 3;",
        "result = (a + b) * (c - d) / 2;",
        "var_1 = 3.14159; var_2 = 2.71828;"
    ]
    
    print("–¢–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã:")
    print("-" * 60)
    
    for i, test in enumerate(test_cases, 1):
        print(f"\\n–ü—Ä–∏–º–µ—Ä {i}:")
        print_tokens(test)
    
    print("\\n" + "=" * 60)
    print("–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º")
    print("–í–≤–µ–¥–∏—Ç–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–∏–ª–∏ 'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞):")
    print("=" * 60)
    
    while True:
        try:
            text = input("\\n>>> ").strip()
            
            if text.lower() == 'exit':
print("–í—ã—Ö–æ–¥ –∏–∑ –ø—Ä–æ–≥—Ä–∞–º–º—ã...")
                break
            
            if not text:
                continue
            
            print_tokens(text)
            
        except KeyboardInterrupt:
            print("\\n\\n–ü—Ä–æ–≥—Ä–∞–º–º–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.")
            break
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞: {e}")

if name == "__main__":
    main()''',
        
        'tests/__init__.py': '',
        
        'tests/test_lexer.py': '''import unittest
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.lexer import Lexer
from src.tokens import TokenType

class TestLexer(unittest.TestCase):
    def test_basic_tokens(self):
        lexer = Lexer("+ - * / = ( ) ;")
        tokens = list(lexer.tokenize())
        
        expected = [
            TokenType.PLUS,
            TokenType.MINUS,
            TokenType.MUL,
            TokenType.DIV,
            TokenType.ASSIGN,
            TokenType.LPAREN,
            TokenType.RPAREN,
            TokenType.SEMI,
            TokenType.EOF
        ]
        
        for token, expected_type in zip(tokens, expected):
            self.assertEqual(token.type, expected_type)
    
    def test_numbers(self):
        lexer = Lexer("123 45.67 0.5 1000")
        tokens = list(lexer.tokenize())
        
        expected_values = [123, 45.67, 0.5, 1000]
        
        for i, (token, expected_value) in enumerate(zip(tokens, expected_values)):
            self.assertEqual(token.type, TokenType.NUMBER)
            self.assertAlmostEqual(token.value, expected_value, places=5)
    
    def test_identifiers(self):
        lexer = Lexer("x y_var _temp var123")
        tokens = list(lexer.tokenize())
        
        expected_values = ["x", "y_var", "_temp", "var123"]
        
        for token, expected_value in zip(tokens[:-1], expected_values):
            self.assertEqual(token.type, TokenType.ID)
            self.assertEqual(token.value, expected_value)
    
    def test_example_1(self):
        lexer = Lexer("x = 5;")
        tokens = list(lexer.tokenize())
        
        expected = [
            (TokenType.ID, "x"),
            (TokenType.ASSIGN, "="),
            (TokenType.NUMBER, 5),
            (TokenType.SEMI, ";"),
            (TokenType.EOF, None)
        ]
        
        for token, (expected_type, expected_value) in zip(tokens, expected):
            self.assertEqual(token.type, expected_type)
            if expected_value is not None:
                self.assertEqual(token.value, expected_value)
    
    def test_example_2(self):
        lexer = Lexer("y = 3.14 * (2 + x);")
        tokens = list(lexer.tokenize())
        
        expected = [
            (TokenType.ID, "y"),
            (TokenType.ASSIGN, "="),
            (TokenType.NUMBER, 3.14),
            (TokenType.MUL, "*"),
            (TokenType.LPAREN, "("),
            (TokenType.NUMBER, 2),
            (TokenType.PLUS, "+"),
            (TokenType.ID, "x"),
            (TokenType.RPAREN, ")"),
            (TokenType.SEMI, ";"),
            (TokenType.EOF, None)
        ]
        
        for token, (expected_type, expected_value) in zip(tokens, expected):
            self.assertEqual(token.type, expected_type)
            if expected_value is not None:
                self.assertEqual(token.value, expected_value)

if name == '__main__':
    unittest.main(verbosity=2)''',
        
        'requirements.txt': '''# –î–ª—è –ø—Ä–æ–µ–∫—Ç–∞ (ANTLR4 –±—É–¥–µ—Ç –≤ —Å–ª–µ–¥—É—é—â–∏—Ö —Ä–∞–±–æ—Ç–∞—Ö)''',
        
        'README.md': '''# üßÆ –ö–µ–π—Å 001: –ê—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–∏–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è

–õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —Ä–∞–±–æ—Ç–∞ 1: –õ–µ–∫—Å–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä

## üìã –û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞
–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –ª–µ–∫—Å–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –¥–ª—è —è–∑—ã–∫–∞ –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–∏—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö, –±–∞–∑–æ–≤—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π –∏ —Ñ—É–Ω–∫—Ü–∏–π –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º –∑–∞–¥–∞–Ω–∏–µ–º.

## üéØ –í—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏
- ‚úÖ –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –≥—Ä–∞–º–º–∞—Ç–∏–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤ –≤ ANTLR4
- ‚úÖ –°–æ–∑–¥–∞–Ω –ª–µ–∫—Å–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
- ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω—ã –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç–∏–ø—ã –ª–µ–∫—Å–µ–º:
  - –ß–∏—Å–ª–æ–≤—ã–µ –ª–∏—Ç–µ—Ä–∞–ª—ã (—Ü–µ–ª—ã–µ –∏ –¥—Ä–æ–±–Ω—ã–µ)
  - –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã
  - –û–ø–µ—Ä–∞—Ç–æ—Ä—ã (+, -, *, /, =)
- –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ (;, (, ))

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

```bash
# –ö–ª–æ–Ω–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π
git clone https://github.com/–í–ê–®_–õ–û–ì–ò–ù/calculator-case-001.git
cd calculator-case-001

# –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç—ã
python tests/test_lexer.py

# –ó–∞–ø—É—Å—Ç–∏—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
python src/main.py
